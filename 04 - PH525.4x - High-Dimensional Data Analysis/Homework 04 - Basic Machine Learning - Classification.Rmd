---
title: "Homework 04 - Basic Machine Learning: Classification"
author: "Alessandro Corradini - Harvard Data Science for Life Science XSeries"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Conditional Expectation Exercises

### Conditional Expectations
Throughout this assessment it will be useful to remember that when our data are 0s and 1s, probabilities and expectations are the same thing. We can do the math, but here is an example in the form of R code:

```{r}
n = 1000
y = rbinom(n,1,0.25)
##proportion of ones Pr(Y)
sum(y==1)/length(y)
##expectaion of Y
mean(y)
```

### Conditional Expectation Exercises #1

Generate some random data to imitate heights for men (0) and women (1):

```{r}
n = 10000
set.seed(1)
men = rnorm(n,176,7) #height in centimeters
women = rnorm(n,162,7) #height in centimeters
y = c(rep(0,n),rep(1,n))
x = round(c(men,women))
##mix it up
ind = sample(seq(along=y))
y = y[ind]
x = x[ind]
```  

Treating the data generated above as the population, if we know someone is 176 cm tall, what it the probability that this person is a woman: $\ \mbox{Pr}(Y=1|X=176)=\mbox{E}(Y|X=176)$?

```{r}
mean(y[x==176])
```

### Conditional Expectation Exercises #2

Now make a plot of $\ E(Y|X=x)$ for x=seq(160,178) using the data generated in Conditional Expectation Exercises #1.

Suppose for each height  you predict 1 (female) if $\ \mbox{Pr}(Y|X=x)>0.5$ and 0 (male) otherwise. What is the largest height for which you predict female ?

```{r}
xs = seq(160,178)
pr =sapply(xs,function(x0) mean(y[x==x0]))
plot(xs,pr)
abline(h=0.5)
abline(v=168)
```

## Smoothing Exercises

### Smoothing Exercises #1

Use the data generated in question 2.6.2

```{r}
n = 10000
set.seed(1)
men = rnorm(n,176,7) #height in centimeters
women = rnorm(n,162,7) #height in centimeters
y = c(rep(0,n),rep(1,n))
x = round(c(men,women))
##mix it up
ind = sample(seq(along=y))
y = y[ind]
x = x[ind]
```  
  
Set the seed at 5, set.seed(5) and take a random sample of 250 individuals from the population like this:

```{r}
set.seed(5)
N = 250
ind = sample(length(y),N)
Y = y[ind]
X = x[ind]
```

Use loess to estimate $\ f(x)=E(Y|X=x)$ using the default parameters. What is the predicted f(168)?

```{r}
fit=loess(Y~X)
predict(fit,newdata=data.frame(X=168))

##Here is a plot
xs = seq(160,178)
Pr =sapply(xs,function(x0) mean(Y[X==x0]))
plot(xs,Pr)
fitted=predict(fit,newdata=data.frame(X=xs))
lines(xs,fitted)
```

### Smoothing Exercises #2

The loess estimate above is a random variable thus we should compute its standard error. Use Monte Carlo simulation to compute the standard error of your estimate of f(168).

Set the seed to 5, set.seed(5) and perform 1000 simulation of the computations performed in question 2.7.1. Report the the SE of the loess based estimate.

```{r}
##plot plots are optional
set.seed(5)
B = 1000
N = 250
xs = seq(160,178)
plot(xs,xs,ylim=c(0,1),type="l")
res = replicate(B,{
  ind = sample(length(y),N)
  Y = y[ind]
  X = x[ind]
  fit=loess(Y~X)
  ##optional plots
  fitted=predict(fit,newdata=data.frame(X=xs))
  lines(xs,fitted)
  estimate = predict(fit,newdata=data.frame(X=168))
  return(estimate)
  })
library(rafalib)
popsd(res)
```

## kNN and Cross Validation Exercises

Load the following dataset

```{r}
library(GSE5859Subset)
data(GSE5859Subset)
```

And define the outcome and predictors. To make the problem more difficult we will only consider autosomal genes:

```{r}
y = factor(sampleInfo$group)
X = t(geneExpression)
out = which(geneAnnotation$CHR%in%c("chrX","chrY"))
X = X[,-out]
```

Note, you will also need to load the following:

```{r}
library(caret)
```

### kNN and Cross Validation Exercises #1

Set the seed to 1 set.seed(1) then use the createFolds function in the caret package to create 10 folds of y.

What is the 2nd entry in the fold 3?

```{r}
set.seed(1)
idx = createFolds(y, k=10)
idx[[3]][2]
sapply(idx,function(ind) table(y[ind]))
```

### kNN and Cross Validation Exercises #2

For the following questions we are going to use kNN. We are going to consider a smaller set of predictors by filtering genes using t-tests. Specifically, we will perform a t-test and select the  genes with the smallest p-values.

Let m = 8 and k = 5 and train kNN by leaving out the second fold idx[[2]]

How many mistakes do we make on the test set? Remember it is indispensable that you perform the ttest on the training data.

```{r}
library(class)
library(genefilter)
m=8
k=5
ind = idx[[2]]
pvals = rowttests(t(X[-ind,]),factor(y[-ind]))$p.val
ind2 = order(pvals)[1:m]
predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
sum(predict!=y[ind])
```

### kNN and Cross Validation Exercises #3

Now run the code for kNN and Cross Validation Exercises #2 for all 10 folds and keep track of the errors. What is our error rate (number of errors divided by number of predictions) ?

```{r}
library(class)
library(genefilter)
m=8
k=5
result = sapply(idx,function(ind){
  pvals = rowttests(t(X[-ind,]),factor(y[-ind]))$p.val
  ind2 = order(pvals)[1:m]
  predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
  sum(predict!=y[ind])
})
sum(result)/length(y)
```

### kNN and Cross Validation Exercises #4

Now we are going to select the best values of  and . Use the expand grid function to try out the following values:

```{r}
ms=2^c(1:11)
ks=seq(1,9,2)
params = expand.grid(k=ks,m=ms)
```

Now use apply or a loop to obtain error rates for each of these pairs of parameters. Which pair of parameters minimizes the error rate?

```{r}
errors = apply(params,1,function(param){
  k =  param[1]
  m =  param[2]
  result = sapply(idx,function(ind){
    pvals = rowttests(t(X[-ind,]),factor(y[-ind]))$p.val
    ind2 = order(pvals)[1:m]
    predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
    sum(predict!=y[ind])
  })
  sum(result)/length(y)
  })
params[which.min(errors),]
##make a plot and confirm its just one min:
errors = matrix(errors,5,11)
library(rafalib)
mypar(1,1)
matplot(ms,t(errors),type="l",log="x")
legend("topright",as.character(ks),lty=seq_along(ks),col=seq_along(ks))
```

### kNN and Cross Validation Exercises #5

Repeat question kNN and Cross Validation Exercises #4 but now perform the t-test filtering before the cross validation. Note how this biases the entire result and gives us much lower estimated error rates.

What is the minimum error rate?

```{r}
pvals = rowttests(t(X),factor(y))$p.val
errors = apply(params,1,function(param){
  k =  param[1]
  m =  param[2]
  result = sapply(idx,function(ind){
    ind2 = order(pvals)[1:m]
    predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
    sum(predict!=y[ind])
  })
  sum(result)/length(y)
  })
min(errors)
##make a plot and compare to previous question
errors = matrix(errors,5,11)
library(rafalib)
mypar(1,1)
matplot(ms,t(errors),type="l",log="x")
legend("topright",as.character(ks),lty=seq_along(ks),col=seq_along(ks))
```

### kNN and Cross Validation Exercises #6

Repeat the cross-validation we performed in question kNN and Cross Validation Exercises #4 but now instead of defining y as sampleInfo$group use:

```{r}
y = factor(as.numeric(format( sampleInfo$date, "%m")=="06"))
```

What is the minimum error rate now?

```{r}
errors = apply(params,1,function(param){
  k =  param[1]
  m =  param[2]
  result = sapply(idx,function(ind){
    pvals = rowttests(t(X[-ind,]),factor(y[-ind]))$p.val
    ind2 = order(pvals)[1:m]
    predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
    sum(predict!=y[ind])
  })
  sum(result)/length(y)
  })
min(errors)
##make a plot and confirm its just one min:
errors = matrix(errors,5,11)
library(rafalib)
mypar(1,1)
matplot(ms,t(errors),type="l",log="x")
legend("topright",as.character(ks),lty=seq_along(ks),col=seq_along(ks))
```