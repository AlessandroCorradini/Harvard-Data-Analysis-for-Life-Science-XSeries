---
title: "Homework 04 - Inference"
author: "Alessandro Corradini - Harvard Data Science for Life Science XSeries"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Inference Review Exercises

The standard error of an estimate is the standard deviation of the sampling distribution of an estimate. In PH525.1x, we saw that our estimate of the mean of a population changed depending on the sample that we took from the population. If we repeatedly sampled from the population, and each time estimated the mean, the collection of mean estimates formed the sampling distribution of the estimate. When we took the standard deviation of those estimates, that was the standard error of our mean estimate.

Here, we aren't sampling individuals from a population, but we do have random noise in our observations Y. The estimate for the linear model terms (beta-hat) will not be the same if we were to re-run the experiment, because the random noise would be different. If we were to re-run the experiment many times, and estimate linear model terms (beta-hat) each time, this is called the sampling distribution of the estimates. If we take the standard deviation of all of these estimates from repetitions of the experiment, this is called the standard error of the estimate. While we are not sampling individuals, you can think about the repetition of the experiment that we are "sampling" new errors in our observation of Y.

### Inference Review Exercises #1

We have shown how to find the least squares estimates with matrix algebra. These estimates are random variables as they are linear combinations of the data. For these estimates to be useful we also need to compute the standard errors.

Here we review standard errors in the context of linear models.

It is useful to think about where randomness comes from. In our falling object example, randomness was introduced through measurement errors. Every time we rerun the experiment a new set of measurement errors will be made which implies our data will be random. This implies that our estimate of, for example, the gravitational constant will change. The constant is fixed, but our estimates are not. To see this we can run a Monte Carlo simulation. Specifically we will generate the data repeatedly and compute the estimate for the quadratic term each time.

```{r}
g = 9.8 ## meters per second

h0 = 56.67

v0 = 0

n = 25

tt = seq(0,3.4,len=n) ##time in secs, t is a base function

y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
```

Now we act as if we didn't know h0, v0 and -0.5*g and use regression to estimate these. We can rewrite the model as y = b0 + b1 t + b2 t^2 + e and obtain the LSE we have used in this class. Note that g = -2 b2.

To obtain the LSE in R we could write:

```{r}
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
```

Given how we have defined A, which of the following is the LSE of g, the acceleration due to gravity (suggestion: try the code in R)?

- 9.8
- A %*% y
- **```-2 * (A %*% y) [3]```**
- A[3,3]

### Inference Review Exercises #2

In the lines of code above, there was a call to a random function rnorm(). This means that each time the lines of code above are repeated, the estimate of g will be different.

Use the code above in conjunction with the function replicate() to generate 100,000 Monte Carlo simulated datasets. For each dataset compute an estimate of g (remember to multiply by -2) and set the seed to 1.

What is the standard error of this estimate?:

```{r}
B = 100000

g = 9.8 ## meters per second

n = 25

tt = seq(0,3.4,len=n) ##time in secs, t is a base function

X = cbind(1,tt,tt^2)

A = solve(crossprod(X))%*%t(X)

set.seed(1)

betahat = replicate(B,{

y = 56.67 - 0.5*g*tt^2 + rnorm(n,sd=1)

betahats = -2*A%*%y

return(betahats[3])

})

sqrt(mean( (betahat-mean(betahat) )^2))
```

### Inference Review Exercises #3

In the father and son height examples we have randomness because we have a random sample of father and son pairs. For the sake of illustration let's assume that this is the entire population:

```{r}
library(UsingR)

x = father.son$fheight

y = father.son$sheight

n = length(y)
```

  
Now let's run a Monte Carlo simulation in which we take a sample of size 50 over and over again. Here is how we obtain one sample:

```{r}
N =  50

index = sample(n,N)

sampledat = father.son[index,]

x = sampledat$fheight

y = sampledat$sheight

betahat =  lm(y~x)$coef
```

  
Use the function replicate to take 10,000 samples.

What is the standard error of the slope estimate? That is, calculate the standard deviation of the estimate from many random samples. Again, set the seed to 1.

```{r}
N = 50

B = 10000

set.seed(1)

betahat = replicate(B,{

index = sample(n,N)

sampledat = father.son[index,]

x = sampledat$fheight

y = sampledat$sheight

lm(y~x)$coef[2]

})

sqrt ( mean( (betahat - mean(betahat) )^2 ))
```

### Inference Review Exercises #4

We are defining a new concept: covariance. The covariance of two lists of numbers X=X1,...,Xn and Y=Y1,...,Yn is mean( (Y - mean(Y))*(X-mean(X) ) ).

Which of the following is closest to the covariance between father heights and son heights

```{r}
Y=father.son$fheight

X=father.son$sheight

mean( (Y - mean(Y))*(X-mean(X) ) )
```

- 0
- -4
- **4**
- 0.5

## Standard Errors Exercises

In the previous assessment, we used a Monte Carlo technique to see that the linear model coefficients are random variables when the data is a random sample. Now we will use the matrix algebra from the previous video to try to estimate the standard error of the linear model coefficients. Again, take a random sample of the father.son heights data:

```{r}
library(UsingR)

x = father.son$fheight

y = father.son$sheight

n = length(y)

N = 50

set.seed(1)

index = sample(n,N)

sampledat = father.son[index,]

x = sampledat$fheight

y = sampledat$sheight

betahat = lm(y~x)$coef
```

The formula for the standard error in the previous video was (the following two lines are not R code):

```
SE(betahat) = sqrt(var(betahat))

var(betahat) = sigma^2 (X^T * X)^-1
```

This is also listed in the standard error book page.

We will estimate or calculate each part of this equation and then combine them.

First, we want to estimate sigma^2, the variance of Y. As we have seen in the previous unit, the random part of Y is only coming from epsilon, because we assume X*beta is fixed. So we can try to estimate the variance of the epsilons from the residuals, the Yi minus the fitted values from the linear model.

### Standard Errors Exercises #1

Note that the fitted values (Y-hat) from a linear model can be obtained with:

```{r}
fit = lm(y ~ x)

fit$fitted.values
```

What is the sum of the squared residuals (where residuals are given by r_i = Y_i - Y-hat_i)?

```{r}
fit = lm(y ~ x)

sum((y - fit$fitted.values)^2)
```

Our estimate of sigma^2 will be the sum of squared residuals divided by (N - p), the sample size minus the number of terms in the model. Since we have a sample of 50 and 2 terms in the model (an intercept and a slope), our estimate of sigma^2 will be the sum of squared residuals divided by 48. Save this to a variable 'sigma2':

sigma2 = SSR / 48

where SSR is the answer to the previous question.

### Standard Errors Exercises #2

Form the design matrix X (note: use a capital X!). This can be done by combining a column of 1's with a column of 'x' the father's heights.

```{r}
X = cbind(rep(1,N), x)
```

Now calculate (X^T X)^-1, the inverse of X transpose times X. Use the solve() function for the inverse and t() for the transpose. What is the element in the first row, first column?

```{r}
solve(t(X) %*% X)
```

### Standard Errors Exercises #3

Now we are one step away from the standard error of beta-hat. Take the diagonals from the (X^T X)^-1 matrix above, using the diag() function. Now multiply our estimate of sigma^2 and the diagonals of this matrix. This is the estimated variance of beta-hat, so take the square root of this. You should end up with two numbers, the standard error for the intercept and the standard error for the slope.

What is the standard error for the slope?

```{r}
fit = lm(y ~ x)

sigma2 = sum((y - fit$fitted.values)^2) / (N - 2)

sqrt(sigma2 * diag(solve(t(X) %*% X)))
```