---
title: "Homework 07 - Calculation of Linear Models"
author: "Alessandro Corradini - Harvard Data Science for Life Science XSeries"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Collinearity Exercises

To answer the first question, consider the following design matrices:

```
A

1 0 0 0
1 0 0 0
1 1 1 0
1 1 0 1

B

1 0 0 1
1 0 1 1
1 1 0 0
1 1 1 0

C

1 0 0 
1 1 2 
1 2 4 
1 3 6 

D

1 0 0 0 0
1 0 0 0 1
1 1 0 1 0
1 1 0 1 1
1 0 1 1 0
1 0 1 1 1

E

1 0 0 0
1 0 1 0
1 1 0 0
1 1 1 1

F

1 0 0 1
1 0 0 1
1 0 1 1
1 1 0 0
1 1 0 0
1 1 1 0
```
 

### Collinearity Exercises #1

Which of the above design matrices does NOT have the problem of collinearity?

```{r}
m = matrix(c(1,1,1,1,0,0,1,1,0,1,0,1,0,0,0,1),4,4)

qr(m)$rank
```

- A
- B
- C
- D
- **E**
- F

Let's use the example from the lecture to visualize how there is not a single best beta-hat, when the design matrix has collinearity of columns.

An example can be made with:

```{r}
sex <- factor(rep(c("female","male"),each=4))
trt <- factor(c("A","A","B","B","C","C","D","D"))
```

The model matrix can then be formed with:

```{r}
X <- model.matrix( ~ sex + trt)
```
And we can see that the number of independent columns is less than the number of columns of X:

```{r}
qr(X)$rank
```

Suppose we observe some outcome, Y. For simplicity we will use synthetic data:

```{r}
Y <- 1:8
```
Now, we will fix the value for two beta's and optimize the remaining betas. We will fix beta_male and beta_D. And then we will find the optimal value for the remaining betas, in terms of minimizing sum((Y - X beta)^2).

The optimal value for the other betas is the one that minimizes:

```
sum( ( (Y - X_male* beta_male - X_D beta_D) - X_R beta_R )^2 )
```

Where X_male is the male column of the design matrix, X_D is the D column, and X_R has the remaining columns.

So all we need to do is redefine Y as Y* = Y - X* beta_male - X** beta_D and fit a linear model. The following line of code creates this  variable Y*, after fixing beta_male to a value 'a', and beta_D to a value, 'b':

```{r}
makeYstar <- function(a,b) Y - X[,2] * a - X[,5] * b
```

Now we'll construct a function which, for a given value a and b, gives us back the the sum of squared residuals after fitting the other terms.

```{r}
fitTheRest <- function(a,b) {
  Ystar <- makeYstar(a,b)
  Xrest <- X[,-c(2,5)]
  betarest <- solve(t(Xrest) %*% Xrest) %*% t(Xrest) %*% Ystar
  residuals <- Ystar - Xrest %*% betarest
  sum(residuals^2)
}
```

### Collinearity Exercises #2

What is the sum of squared residuals when the male coefficient is 1 and the D coefficient is 2, and the other coefficients are fit using the linear model solution?

```{r}
fitTheRest(1,2)
```

We can apply our function fitTheRest to a grid of values for beta_male and beta_D, using the expand.grid function in R. expand.grid takes two vectors and returns a matrix with rows containing all possible combination. Try it out:

```{r}
expand.grid(1:3,1:3)
```

We can run fitTheRest on a grid of values, using the following code (the Vectorize() is necessary as outer() requires only vectorized functions):

```{r}
betas = expand.grid(-2:8,-2:8)

rss = apply(betas,1,function(x) fitTheRest(x[1],x[2]))
```

### Collinearity Exercises #3

Which of the following pairs of values minimizes the RSS?

```{r}
themin= min(rss)
betas[which(rss==themin),]
```

- 8, -2
- 6, 0
- 1, 5
- **All of the above. There is no single minimum.**

It's fairly clear from just looking at the numbers, but we can also visualize the sum of squared residuals over our grid with the imagemat() function from rafalib:

```{R}
library(rafalib)

## plot the pairs what are minimum

themin=min(rss)

plot(betas[which(rss==themin),])
```

There is clearly not a single beta which optimizes the least squares equation, due to collinearity, but an infinite line of solutions which produce an identical sum of squares values.

## QR Exercises

We will use the spider dataset to try out the QR decomposition as a solution to linear models. Load the full spider dataset, by using the code in the Interactions and Contrasts book page. Run a linear model of the friction coefficient with two variable (no interactions):

```{r}
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/spider_wolff_gorb_2013.csv"
filename <- "spider_wolff_gorb_2013.csv"
library(downloader)
if (!file.exists(filename)) download(url, filename)
spider <- read.csv(filename, skip=1)

fit <- lm(friction ~ type + leg, data=spider)
```

The solution we are interested in solving is:

```{r}
betahat <- coef(fit)
```

So for our matrix work, 

```{r}
Y <- matrix(spider$friction, ncol=1)

X <- model.matrix(~ type + leg, data=spider)
```

In the material on QR decomposition, we saw that the solution for beta is:

R beta = Q^T Y

### QR Exercises #1

What is the first row, first column element in the Q matrix for this linear model?

```{r}
Q = qr.Q(qr(X))
Q[1]
```

### QR Exercises #2

What is the first row, first column element in the R matrix for this linear model?

```{r}
R = qr.R(qr(X))
R[1]
```

### QR Exercises #3

What is the first row, first column element of Q^T Y (use crossprod() as in the book page)

```{r}
head(crossprod(Q, Y))
```

Finally convince yourself that the QR gives the least squares solution by putting all the pieces together:

R^-1 (Q^T Y) compared to betahat