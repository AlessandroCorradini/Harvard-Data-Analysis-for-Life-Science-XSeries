---
title: "Homework 02 - Error rates and Procedures"
author: "Alessandro Corradini - Harvard Data Science for Life Science XSeries"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Error Rates and Procedures Exercises

In this assessment we hope to help you further grasp the concept that p-values are random variables and start laying the ground work for the development of procedures that control error rates. The calculations to compute error rates require us to understand the random behavior of p-values.

We are going to ask you to perform some calculations related to introductory probability theory. One particular concept you need to grasp is statistical independence. You also will need to know that the probability of two random events that are statistically independent occurring is P( A and B) = P(A)P(B). Note that this is a consequence of the more general formula P(A and B) = P(A) P(B | A )

### Error Rates and Procedures Exercises #1

Assume the null is true and denote the p-value you would get if you ran a test as P. Define the function $\ f(x) = \mbox{Pr}(P \leq x)$ What does f(x) look like?

- a uniform distribution
- **the identity line**
- a constant at 0.05
- P is not a random value

### Error Rates and Procedures Exercises #2

In the previous assessment we saw how the probability of incorrectly rejecting the null for at least one of 20 experiments for which the null is true is well over 5%. Now let's consider a case in which we run thousands of tests as we would do in a high throughput experiment.

We previously learned that under the null, the probability of a p-value < p is p. If we run 8,793 independent tests, what is the probability of incorrectly rejecting at least one of the null hypotheses?

```{r}
B<-1000
minpval <- replicate(B, min(runif(8793,0,1))<0.05)
mean(minpval>=1)
``` 

### Error Rates and Procedures Exercises #3 (Sidak's procedure)

Suppose we need to run 8,793 statistical tests and we want to make the probability of a mistake very small, say 5%. Using the answer to exercise #2, how small do we have to change the cutoff, previously 0.05, to lower our probability of at least one mistake to be 5%.

```{r}
B=10000
cutoffs = 10^seq(-7,-4,0.1) ##we know it has to be small
prob = sapply(cutoffs,function(cutoff){
    minpval =replicate(B, min(runif(8793,0,1))<=cutoff)
    mean(minpval>=1)
    })
cutoffs[which.min(abs(prob-0.05))]
```

## Bonferroni Correction Exercises

This assessment should help you understand the concept of a error controlling procedure. You can think of it as defnining a set of instructions, such as "reject all the null hypothesis for  for which p-values < 0.0001" or "reject the null hypothesis for the 10 features with smallest p-values". Then, knowing the p-values are random variables, we use statistical theory to compute how many mistakes, on average, will we make if we follow this procedure. More precisely we commonly bounds on these rates, meaning that we show that they are smaller than some predermined value.

As described in the video, we can compute different error rates. The FWER tells us the probability of having at least one false positive. The FDR is the expected rate of rejected null hypothesis.

Note 1: the FWER and FDR are not procedures but error rates. We will review procedures here and use Monte Carlo simulations to estimate their error rates.

Note 2: We sometimes use the colloquial term "pick genes that" meaning "reject the null hypothesis for genes that."

### Bonferroni Correction Exercises #1 (Bonferonni versus Sidak)

So we have learned about the family wide error rate FWER. This is the probability of incorrectly rejecting the null at least once. Using the notation in the video this probability is written like this: $\ \mbox{Pr}(V>0)$.

What we want to do in practice is choose a procedure that guarantees this probability is smaller than a predetermined value such as 0.05. Here we keep it general and instead of 0.05 we use $\ \alpha$

We have already learned that the procedure "pick all the genes with p-value <0.05" fails miserably as we have seen that $\ Pr(V>0)  \approx 1$. So what else can we do?

The Bonferroni procedure assumes we have computed p-values for each test and asks what constant k should we pick so that the procedure "pick all genes with p-value less than k" has $\ \mbox{Pr}(V>0) = 0.05$. And we typically want to be conservative rather than lenient, so we accept a procedure that has $\ \mbox{Pr}(V>0) \leq 0.05$.

So the first result we rely on is that this probability is largest when all the null hypotheses are true:

$\ \mbox{Pr}(V>0) \leq \mbox{Pr}(V>0 \, | \, \mbox{all nulls are true})$

or using the notation in the video:

$\\mbox{Pr}(V>0) \leq \mbox{Pr}(V>0 \, | \, m_1=0)$

In an earlier assessment we showed that if the tests are independent then

$\ \mbox{Pr}(V>0 \, | \, m_1=0) = 1-(1-k)^m$

And we pick k so that $\1-(1-k)^m = \alpha \implies k = 1-(1-\alpha)^{1/m}$
Now, this requires the tests to be independent. The Bonferroni procedure does not make this assumption and as we saw in the video if we set  this procedure has the property that .

In R define

```{r}
alphas <- seq(0,0.25,0.01)
par(mfrow=c(2,2))
for(m in c(2,10,100,1000)){
  plot(alphas,alphas/m - (1-(1-alphas)^(1/m)),type="l")
  abline(h=0,col=2,lty=2)
}
```

Make a plot of  and  for various values of m>1.

Which procedure is more conservative (picks less genes, i.e. rejects less null hypothesis): Bonferroni's or Sidak's?


- The are the same
- **Bonferroni's**
- Depends on m
- Sidak's

### Bonferroni Correction Exercises #2 (Monte Carlo Simulation)

Monte Carlo simulation. To simulate the p-value results of, say, 8,793 t-tests for which the null is true we don't actual have to generate the original data. As we learned in class we can generate p-values from a uniform distribution like this:

```{r}
pvals <- runif(8793,0,1)
```

Using what we have learned, set the cutoff using the Bonferroni correction that guarantees an FWER lower than 0.05 and report back the FWER. Set the seed at 1,set.seed(1) and run 10,000 simulation. Report the Monte Carlo estimate of the FWER below.

```{r}
set.seed(1)
B <- 10000
m <- 8793
alpha <- 0.05
pvals <- matrix(runif(B*m,0,1),B,m)
k <- alpha/m
mistakes <- rowSums(pvals<k) 
mean(mistakes>0)
```

### Bonferroni Correction Exercises #3

Using the same seed repeat the above for Sidak's cutoff.

Report the FWER below.

```{r}
set.seed(1)
B <- 10000
m <- 8793
alpha <- 0.05
pvals <- matrix(runif(B*m,0,1),B,m)
k <- (1-(1-alpha)^(1/m))
mistakes <- rowSums(pvals<k) 
mean(mistakes>0)
```

## FDR Exercises

In this assessment we will define error controlling procedures for experimental data. We will make list of genes based on q-values. We will also assess your understanding of false positives rates and false negative rates by asking you to create a Monte Carlo simulation.

You will need to install the following libraries:

```{r}
library(devtools)
library(rafalib)
install_github("genomicsclass/GSE5859Subset")
install_bioc("genefilter")
install_bioc("qvalue")
```

### FDR Exercises #1

Load the gene expression data

```{r}
library(GSE5859Subset)
data(GSE5859Subset)
```

We are interested in comparing gene expression between the two groups defined in the sampleInfo table.

Compute a p-value for each gene using the function rowttests from the genefilter package in Bioconductor.

```{r}
library(genefilter)
?rowttests
```

How many genes have p-values smaller than 0.05?

```{r}
g <- factor(sampleInfo$group)
pvals = rowttests(geneExpression,g)$p.value
sum(pvals<0.05)
```

### FDR Exercises #2

Apply the Bonferroni correction to the p-values obtained in question #1 to achieve a FWER of 0.05. How many genes are called significant under this procedure?

```{r}
k = 0.05/length(pvals)
sum(pvals<k)
```

### FDR Exercises #3

Note that the FDR is a property of a list of features, not each specific feature. The q-value relates FDR to an individual feature. To define the q-value we order features we tested by p-value then compute the FDRs for a list with the most significant, the two most significant, the three most significant, etc... The FDR of the list with the, say, m most significant tests is defined as the q-value of the m-th most significant feature. In other words, the q-value of a feature, is the FDR of the biggest list that includes that gene.

In R, we can compute the q-value using the p.adjust function with the FDR option. Read the help file for p.adjust and then, for our gene expression dataset, compute how many genes achieve an FDR < 0.05

```{r}
g = factor(sampleInfo$group)
pvals = rowttests(geneExpression,g)$p.value
fdr = p.adjust(pvals,method="fdr")
sum(fdr<0.05)
```

### FDR Exercises #4

Now use the qvalue function, in the Bioconductor qvalue package, to estimate q-values using the procedure described by Storey.

Using this estimate how many genes have q-values below 0.05?

```{r}
library(qvalue)
res = qvalue(pvals)
qvals = res$qvalues
sum(qvals<0.05)
```

### FDR Exercises #5

Read the help file for qvalue and report the estimated proportion of genes for which the null hypothesis is true 

```{r}
qvalue(pvals)$pi0
```

### FDR Exercises #6

Note that we have the number of genes passing the q-value <0.05 threshold is larger with the qvalue function than the p.adjust difference.

Why is this the case? Make a plot of the ratio of these two estimates to help answer the question.

```{r}
plot(qvalue(pvals)$qvalue/p.adjust(pvals,method="fdr"))
abline(h=qvalue(pvals)$pi0,col=2)
```

```{r}
hist(pvals,breaks=seq(0,1,len=21))
expectedfreq <- length(pvals)/20 #per bin
abline(h=expectedfreq*qvalue(pvals)$pi0,col=2,lty=2)
```

- One of the two procedures is flawed.
- The two functions are estimating different things.
- **The qvalue function estimates the proportion of genes for which the null hypothesis is true and provides a less conservative estimate**
- The qvalue function estimates the proportion of genes for which the null hypothesis is true and provides a more conservative estimate

### FDR Exercises #7

Note that this is an advanced question and that you can ask questions in the discussion forum.

Create a Monte Carlo Simulation in which you simulate measurements from 8,793 genes for 24 samples: 12 cases and 12 controls.

```{R}
n <- 24
m <- 8793
mat <- matrix(rnorm(n*m),m,n)
```

Now for 500 genes, there is a difference of 2 between cases and controls:

```{r}
delta <- 2
positives <- 500
mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
```

So the null hypothesis is true for 8793-500 genes. Using the notation from the videos m=8793, m0=8293 and m1=500

Set the seed at 1, set.seed(1) and run this experiment 1,000 times with a Monte Carlo simulation. For each instance compute p-values using a t-test (using rowttests in the genefilter package) and create three lists of genes using:

- Bonferroni correction to achieve an FWER of 0.05,
- p-adjust estimates of FDR to achieve an FDR of 0.05, and
- qvalue estimates of FDR to to achieve an FDR of 0.05.

For each of these three lists compute the number of false positives in the list and the number of false negatives: genes not in the list that should have been because the null hypothesis is not true (we added 2 to the controls to create the cases).

What is the false positive rate (false positives divided by m0) if we use Bonferroni?

```{r}
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FP1
  })
``` 

### FDR Exercises #8

From the same Monte Carlo simulation as in the question above, what is the false negative rate if we use Bonferroni?

```{r}
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
  c(FP1,FN1)
  })
```  

### FDR Exercises #9

From the same Monte Carlo simulation as in question #7, what is the false positive rate if we use q-values from p.adjust?

```{r}
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
  #p.adjust q-value
  qvals1 = p.adjust(pvals,method="fdr")
  FP2 <- sum(qvals1[-(1:positives)]<=0.05)
  c(FP1,FN1,FP2)
  })
```  

### FDR Exercises #10

From the same Monte Carlo simulation as in question #7, what is the false negative rate if we use q-values from p.adjust?

```{r}
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
   #p.adjust q-value
  qvals1 = p.adjust(pvals,method="fdr")
  FP2 <- sum(qvals1[-(1:positives)]<=0.05)
  FN2 <- sum(qvals1[1:positives]>0.05)
  c(FP1,FN1,FP2,FN2)
  })
```  

### FDR Exercises #11

From the same Monte Carlo simulation as in question #7, what is the false positive rate if we use q-values from qvalue function?

```{r}
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
   #p.adjust q-value
  qvals1 = p.adjust(pvals,method="fdr")
  FP2 <- sum(qvals1[-(1:positives)]<=0.05)
  FN2 <- sum(qvals1[1:positives]>0.05)
  #qvalue q-value
  qvals2 = qvalue(pvals)$qvalue
  FP3 <- sum(qvals2[-(1:positives)]<=0.05)
  c(FP1,FN1,FP2,FN2,FP3)
  })
```  
  
### FDR Exercises #12

From the same Monte Carlo simulation as in question #7, what is the false negative rate if we use q-values from qvalue function?

```{r}
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
   #p.adjust q-value
  qvals1 = p.adjust(pvals,method="fdr")
  FP2 <- sum(qvals1[-(1:positives)]<=0.05)
  FN2 <- sum(qvals1[1:positives]>0.05)
  #qvalue q-value
  qvals2 = qvalue(pvals)$qvalue
  FP3 <- sum(qvals2[-(1:positives)]<=0.05)
  FN3 <- sum(qvals2[1:positives]>0.05)  
  c(FP1,FN1,FP2,FN2,FP3,FN3)
  })
``` 