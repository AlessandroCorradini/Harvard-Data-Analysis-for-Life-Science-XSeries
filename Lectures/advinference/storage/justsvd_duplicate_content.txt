---
layout: page
title: Principal component analysis and Singular value decomposition 
---

```{r options, echo=FALSE}
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

We have measurements for $m$ genes and $n$ samples in a matrix $Y_{m\times n}$. Suppose we 
suspect that a batch effect is responsible for most the variability. We know that some samples fall in one batch and the rest in an other, but we don't know which sample is in which batch. Can we discover the batch? If we assume that many genes will have a different average in batch compared to the other then we can quantify this problem as searching for the separation that makes many of these differences in average large. TO simplify and illustrate further assume $n/2$ samples are in one batch and $n/2$ in the other but we dont know whcih. Can we find the separation?

Assume the gene in row $i$ is affected by batch. Then 
$$
(Y_{i1}, \dots, Y_{in}) (v_1,\dots,v_n) = \sum_{i=1}^n v_i Y_{in}'
$$
with each $v_i$ either $1/(n/2)$ or $-1/(n/2)$ will give us the average difference between each batch for gene $i$, call it $\m_i$. Because we think the batch effect many genes then we want to find the vector $v=(v_1\dots,v_n)$ that maximizes the variace of $m_1,\dots,m_n$.

There is actually a nice mathematical result that can help us find this vector. In fact, if we let $v$ be any vector with standard deviation 1, then the $v$ that maximizes the variance of $Y_i v$ is called the first _principal component_ directions or eigen vector. The vectors of "differences" $Y_i v$, $i=1,\dots,n$ is the first principal component and below we will refer to it as $v_1$ 

Now, suppose we think there is more unwanted variability affecting several genes. We can subtract the first principal component from $Y_{m\time n}$, $r_{m\times n}=Y_{m \times n} - Y_{m \times n} v_1 v_1'$ we can then find the vector $v_2$ that results in the most variable vector  $r_{m\times n} v_2$. We continue this way until to obtain $n$ eigen vectors $V_{n\times n} = (v_1,\dots v_n)$. 

## Singular value decomposition (SVD)

The SVD is a very powerful mathematical result that gives us an algorithm to write a matrix in the following way:

$
Y_{m\times n} = U_{m\ times n} D_{n \times n} Vâ€™_{n \times n}
$

With the columns of $V$ the matrix with columns the eigen vectors defined above. The matrices $U$ and $V$ are _orthogonal_ meaning that 
with $U_i'U_i=1$ and $U_i'U_i$=0 where $U_i$ and $U_j$ are $i$th and $j$th columns of 1. 

Notice this matrix:
$$
Y_{m\times n} V = U_{m \times n} D_{n\times n}
$$
has the principal coponents as columns and that the standard deviation of the $i$ principal component is  $D_{i,i}/n$:
$$
(Y_{m\times n} V)'(Y_{m\times n} V) = D_{n\times n} U'_{m\times n} U_{m\times n} = D^2_{n\times n}
$$

## Example
Let's consider a simple example. Suppose we have the heights of identical twin pairs in an $m\times 2$ matrix. We are asked to 

```{r}
library(MASS)
set.seed(1)
y=mvrnorm(1000,c(0,0),3^2*matrix(c(1,.9,.9,1),2,2))
mypar(1,1)
plot(y,xlab="Twin 1 (inches away from avg)",ylab="Twin 2 (inches away from avg)")
```


Transmitting the two heights seems inefficient given how correlated they. If we tranmist the pricipal components instead we save money. Let's see how:

```{r}
s=svd(y)
plot(s$u[,1]*s$d[1],s$u[,2]*s$d[2],ylim=range(s$u[,1]*s$d[1]),xlab="First PC",ylab="Second PC")
```








